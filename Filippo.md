
# C. Appunti e Note

Le Immagini nel Dataset JUMP sono in formato .TIFF Hyperstack, con 4 canali per ogni immagine che rappresentano RGB + Alpha.

Noise2Void impara direttamente dalle Noisy Images, senza bisogno di Clean Images (like Auto-Encoders) o Multiple Noisy Images (like Noise2Noise). \
Assunzione:
    - Il Segnale ha una struttura, il Rumore non ha una struttura (ovvero i pixel di s non sono indipendenti mentre quelli di n sÃ¬)
      => NecessitÃ  di UNSTRUCTURED NOISE
    
Questo ci consente, guardando i pixel vicini, di predirre il segnale e non il rumore associato.

Si basa su una Blind-Spot Network, differente rispetto alle CNN tradizionali poichÃ© il Campo Recettivo non considera il pixel centrale. Esso Ã¨ sostituito da un valore estratto da un vicino scelto casualmente dal campo recettivo. Questo Ã¨ l'Input Patch.

La predizione (Target Patch) Ã¨ l'Input Patch modificata, ma avente come pixel centrale il precedente pixel centrale del segnale originale.

Il procedimento Ã¨ poi quello tradizionale delle Reti Neurali, con minimizzazione dell'errore...

Vantaggi di N2V:
    + Richiede semplicemente Noisy Images
    + E' molto rapido, vedi confronto con BM3D


## Architettura di N2V

Come si arriva all'Architettura di N2V:

### 1. Perceptron

> IDEA: Simulate a humna neuron

The perceptron is a basic neural network unit that models the functioning
of a biological neuron. It was developed by Frank Rosenblatt in the late 1950s
and has since been widely used in various pattern recognition and classification
tasks. The perceptron takes one or more inputs, which are features variables
and computes the weighted sum of the inputs, and applies an activation function to produce the output. 
The output is determined based on whether the
sum of the weighted inputs is greater than a certain threshold. It is a building
block for more complex neural networks, such as multi-layer perceptrons

The perceptron consists of 4 parts.

Input values or One input layer
Weights (and Bias)
Net sum
Activation Function -> Per le nostre applicazioni, ReLU

The choice of an
activation function Ïƒ is purely empirical such that there is always a room for
an alternative choice given the problem and the performance of a model using
a particular activation function Ïƒ

[Check out
https://www.researchgate.net/publication/369921211_Deep_Learning_Architectures
https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53
https://en.wikipedia.org/wiki/Rectifier_(neural_networks)
]


### 2. Multi-Layer Perceptron (Artifical Neural Network or Feed-Forward Neural Network)

> IDEA: Approximate a complex function

Deep Feed-Forward networks, commonly known as neural networks or multilayer perceptrons (MLPs), are considered the most representative models of
deep learning. Feed-Forward Neural Networks are a type of artificial neural
network that are widely used in machine learning and deep learning applications. A Feed-Forward network aims to approximate a function f*, which could
be the mapping of an input x to a category y. The network creates a mapping
y = f(x; Î¸) and optimizes the parameters Î¸ to obtain the best function approximation fâˆ—. 
In this case Î¸ is the variable representing learning parameters
of the function fâˆ—. 
These networks are called Feed-Forward because they
are designed to process information in a forward direction, from the input layer
through the hidden layers to the output layer. The input layer receives the input
data, which is then processed by the hidden layers using a set of weights and
biases, and the output is generated by the output layer. The model does not
have any internal connections that allow the output to be fed back into itself
in contrast to Recurrent Neural Networks. The model can be represented
by a directed acyclic graph that illustrates the composition of functions.



[Check out
https://www.researchgate.net/publication/369921211_Deep_Learning_Architectures
]

### 3. Convolutional Neural Network

> IDEA: Image Recognition and Classification

Although Feed-Forward Neural Networks are capable of handling image recognition tasks, Convolutional Neural Networks (CNNs)
are more suitable for these types of problems due to their superior performance.
Unlike Feed-Forward Neural Networks, which utilize vectors and matrices as
learning parameters, CNNs use filters or volumes as learning parameters and
convolutions or volumes to process the data from the filters. Although CNNs
are designed specifically for image recognition tasks, they still rely on FeedForward Neural Networks as a key component in their architecture. In most
cases, FFNs form the last layers of a CNN, where they take in flattened data,
which is a transformed version of the initial input volume into a simple vector, and perform classification tasks on the images.

Convolutional Neural Network is a type of neural networks that have been used
in image classification tasks, and object recognition for a long period of time.
They are using the same architecture as Feed-Forward Neural Networks, with
the only difference in the introduction of convolutions as layers, filters as weights.
Beyond that they use back-propagation as Feed-Forward Neural Networks, activation functions, optimization functions and gradient descent as a their building
blocks.

As it was mentioned earlier, the Feed-Forward Neural Networks can also solve
image classification problems, but usually the training process take longer and
the accuracy is quite high on the real test datasets. The reason behind that is
the complexity of the objects represented in an image. Complexity includes an
objectâ€™s class features such as shape of an object, the distance between one point
to the another, the color, and so on. Therefore, feed-forward neural networks
take an image of size (N, N) as a flatten version of it. The flatten version of a
matrix is a vector V of size (N, 1).

In pratica prima applico CNN, con cui viene comodo il formato matriciale, poi la appiattisco e la mando alla FNN.

The output of a Convolutional Neural Network (CNN) is typically a set of predictions based on the input data, which is often an image or a sequence of images. Hereâ€™s a breakdown of the process leading to the output:
Input Layer: The CNN receives an input, usually an image represented in three dimensions (width, height, and depth for color channels).
Convolutional Layers: These layers apply filters (kernels) to the input image to extract features. Each filter detects specific patterns such as edges or textures. The output from this layer is known as a feature map, which highlights the presence of detected features in the image.
Activation Layers: After convolution, activation functions (like ReLU or sigmoid) are applied to introduce non-linearity, allowing the network to learn complex patterns.
Pooling Layers: These layers reduce the dimensionality of the feature maps while retaining important information, typically using operations like max pooling or average pooling. This helps in making the network more computationally efficient and reduces overfitting.
Flattening: The pooled feature maps are flattened into a one-dimensional vector to prepare them for input into fully connected layers1.
Fully Connected Layers: These layers take the flattened feature maps and perform classification or regression tasks by computing outputs based on learned weights from previous layers.
Output Layer: Finally, the output from the fully connected layer is passed through an activation function (such as softmax for multi-class classification) to produce probability scores for each class. This gives the final prediction, indicating which class the input image belongs to.
In summary, the output of a CNN is a probability distribution across different classes for classification tasks or specific values for regression tasks, depending on its design and intended application.

If the patch size is the same as that of the image it will be a regular neural network. Because of this small patch, we have fewer weights. 

NOTA: i Dense Layers (la FNN) alla fine della CNN richiede che l'input venga portato ad una dimensione fissa. 
Questo Ã¨ evidente poichÃ© una matrice W di pesi MxN puÃ² essere applicata solamente ad un vettore di input di lunghezza N, per ottenere
un vettore di output di lunghezza M. (Si noti: y = Wx + b, non y = xW + b) 

[Check out
https://www.researchgate.net/publication/369921211_Deep_Learning_Architectures
https://www.geeksforgeeks.org/introduction-convolution-neural-network/
https://www.andreaprovino.it/convolutional-neuralnetwork
]

### 4. Fully Convolutional Network

> IDEA: Image Semantic Segmentation
 
CONVOLUTIONAL networks are driving advances in
recognition. Convnets are not only improving for
whole-image classification, but also making
progress on local tasks with structured output. These include advances in bounding box object detection,
part and keypoint prediction, and local correspondence.
The natural next step in the progression from coarse to
fine inference is to make a prediction at every pixel. Prior
approaches have used convnets for semantic segmentation, in which each pixel is
labeled with the class of its enclosing object or region, but
with shortcomings that this work addresses

Semantic segmentation faces an inherent tension between semantics and location: global information resolves
what while local information resolves where. What can be
done to navigate this spectrum from location to semantics?
How can local decisions respect global structure? It is not
immediately clear that deep networks for image classification yield representations sufficient for accurate, pixelwise
recognition

Una FCN o Fully Convolutional Network Ã¨ una deep neural network (rete neurale profonda) che supera le limitazioni delle convenzionali CNN eliminando il dense layer in favore di 1Ã—1 convolutional layers.
Lâ€™assenza dei fully connected layers consente alle FCN Networks di:
 - elaborare immagini di diverse dimensioni, cosa non possibile con le strutture fisse convenzionali delle CNN.
 - avere una struttura piÃ¹ snella (lower parameters) e aumentare quindi la velocitÃ  computazionale, riducendo la latenza (low latency).

Feature	                        CNN (Convolutional Neural Network)	                                            FCN (Fully Convolutional Network)
Fully Connected Layers	        Yes, after convolution and pooling layers	                                    No, uses convolutional layers throughout the network
Spatial Information	            Spatial information lost after flattening (fixed-size output)	                Retains spatial information (output size depends on input)
Output	                        Fixed output size (e.g., class labels)	                                        Variable output size (e.g., pixel-level predictions in segmentation)
Input Size	                    Fixed input size (needs resizing)	                                            Variable input size (can handle different input sizes)
Use Cases	                    Image classification, regression	                                            Image segmentation, object localization, dense predictions

Instead of using fully connected layers, we can replace them with 1x1 convolutional layers. Here's why this works:

1x1 convolutions are similar to fully connected layers in the sense that each 1x1 filter connects to all channels at each spatial location. This allows the network to learn class-specific features at each pixel location, similar to how FC layers would learn overall image features, but in a more spatially-aware way.
This is done after the convolutional layers, and it allows the network to make pixel-wise predictions for each location in the image.
Now, instead of reducing the image to a single class label, the network produces an output map that retains the spatial dimensions of the original image. But initially, after passing through convolutions, the output will still be smaller than the input due to operations like max pooling or stride convolutions.
etup
Let's say your input feature map has dimensions 32x32x128. This means the spatial dimensions of the image are 32x32, and there are 128 channels (depth).

Input dimensions: 
32Ã—32Ã—128
32x32 are the spatial dimensions (height and width).
128 is the number of channels (depth).
1x1 Convolution Operation
A 1x1 convolution applies a small 1x1 filter across the entire spatial grid (32x32), but with a depth that spans the number of input channels. Essentially, the filter "sees" the entire set of channels at each spatial location (pixel), and combines them in a weighted way.

Filter size: 
1Ã—1Ã—128 (this is the size of the filter applied at each pixel location, with depth equal to the number of input channels).
Stride: Typically, stride=1 for 1x1 convolutions.
Padding: Padding is usually not needed for 1x1 convolutions since you're applying it only on the spatial dimensions (height and width).
How the 1x1 Convolution Works
For each pixel (location) in the spatial grid (32x32), the convolution filter looks at all the channels in the input feature map. The 1x1 filter has 128 weights, one for each channel, and these weights are applied to the respective channels at that spatial location.

Each spatial location (pixel) in the input feature map is processed by the 1x1 filter. The filter applies a weighted sum across all 128 channels at that pixel.
Weight matrix for each spatial location: For each pixel, you apply a filter that is 
1Ã—1Ã—128, so each pixel gets processed by a weight matrix that takes all 128 input channels into account.
Output of 1x1 Convolution
The output of this operation depends on how many filters (weights) you have in the 1x1 convolution layer.
If you have M filters in the 1x1 convolution, the output will have M channels for each spatial location.
Let's break down what happens during the 1x1 convolution:

Each of the 128 input channels contributes to the final output at that pixel location (spatial location).
If you apply M filters in the 1x1 convolution, each filter will generate one output value per pixel. So, for each spatial location, you will get M values, one for each filter.
Example: 
32Ã—32Ã—128 â†’ 32Ã—32Ã—M
Input: 
32Ã—32Ã—128 (height x width x channels).
1x1 Convolution Filters: 
1Ã—1Ã—128 (for each pixel).
Number of Filters: Suppose you use M filters in the 1x1 convolution layer.
The result after the 1x1 convolution will be an output feature map of size 32x32xM, where M is the number of filters you have in the 1x1 convolution. Each filter produces one output value per spatial location, so the output has the same spatial dimensions (32x32) as the input, but the number of channels is now M instead of 128.

The Process at Each Spatial Location
Take each pixel in the input feature map (of size 32Ã—32).
Look at the 128 channels (depth) at that pixel.
Apply each filter: Each filter has a set of 128 weights (one for each input channel at that pixel location). The filter computes a weighted sum of all the 128 input channels at that pixel.
Output: The result is one output value for that pixel location, which is repeated across all pixels in the spatial grid.
Key Points:
A 1x1 convolution does not change the spatial dimensions (height and width), but it changes the depth (the number of channels).
It combines features from all the channels at each pixel and outputs a new set of features.
The number of filters M determines how many output channels you have for each spatial location.
The 1x1 convolution is used to learn complex combinations of features without changing the spatial dimensions of the input.

3. Fusing the Output
After going through conv7 as below, the output size is small, then 32Ã— upsampling is done to make the output have the same size of input image. But it also makes the output label map rough. And it is called FCN-32s:

This is because, deep features can be obtained when going deeper, spatial location information is also lost when going deeper. That means output from shallower layers have more location information. If we combine both, we can enhance the result.

To combine, we fuse the output (by element-wise addition):

FCN-16s: The output from pool5 is 2Ã— upsampled and fuse with pool4 and perform 16Ã— upsampling. Similar operations for FCN-8s as in the figure above.

FCN-32s result is very rough due to loss of location information while FCN-8s has the best result.

This fusing operation actually is just like the boosting / ensemble technique used in AlexNet, VGGNet, and GoogLeNet, where they add the results by multiple model to make the prediction more accurate. But in this case, it is done for each pixel, and they are added from the results of different layers within a model.

FCN-16s and 8s pose the intuition for the U-Net skip connections


[Check out
https://arxiv.org/pdf/1605.06211v1
https://www.andreaprovino.it/fcn
https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1
https://medium.com/@mohit_gaikwad/overview-fully-convolutional-network-for-semantic-segmentation-b4ef92eeb8c4
]

### 5. U-Net

> IDEA: Make Semantic Segmentation more effective and efficient, with less training data => Specific for Medical Datasets

From the original paper:

In this paper, we build upon a more elegant architecture, the so-called â€œfully
convolutional networkâ€. We modify and extend this architecture such that it
works with very few training images and yields more precise segmentations; see
Figure 1

One important modification in our architecture is that in the upsampling
part we have also a large number of feature channels, which allow the network
to propagate context information to higher resolution layers. As a consequence,
the expansive path is more or less symmetric to the contracting path, and yields
a u-shaped architecture. The network does not have any fully connected layers
and only uses the valid part of each convolution, i.e., the segmentation map only
contains the pixels, for which the full context is available in the input image.
This strategy allows the seamless segmentation of arbitrarily large images by an
overlap-tile strategy (see Figure 2). To predict the pixels in the border region
of the image, the missing context is extrapolated by mirroring the input image.
This tiling strategy is important to apply the network to large images, since
otherwise the resolution would be limited by the GPU memory.

As for our tasks there is very little training data available, we use excessive
data augmentation by applying elastic deformations to the available training images. This allows the network to learn invariance to such deformations, without
the need to see these transformations in the annotated image corpus. This is
particularly important in biomedical segmentation, since deformation used to
be the most common variation in tissue and realistic deformations can be simulated efficiently. 

Network Architecture:

The network architecture is illustrated in Figure 1. It consists of a contracting
path (left side) and an expansive path (right side). The contracting path follows
the typical architecture of a convolutional network. It consists of the repeated
application of two 3x3 convolutions (unpadded convolutions), each followed by
a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2
for downsampling. At each downsampling step we double the number of feature
channels. Every step in the expansive path consists of an upsampling of the
feature map followed by a 2x2 convolution (â€œup-convolutionâ€) that halves the
number of feature channels, a concatenation with the correspondingly cropped
feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in
every convolution. At the final layer a 1x1 convolution is used to map each 64-
component feature vector to the desired number of classes. In total the network
has 23 convolutional layers

The u-net architecture achieves very good performance on very different biomedical segmentation applications. Thanks to data augmentation with elastic deformations,
it only needs very few annotated images and has a very reasonable training time

----------

From other sites:

The main idea is to supplement a usual contracting network by successive layers, where pooling operations are replaced by upsampling operators. Hence these layers increase the resolution of the output. A successive convolutional layer can then learn to assemble a precise output based on this information.

The other fundamental idea is Shortcut Connections

La caratteristica distintiva di una rete neurale U-Net Ã¨ costituita dalle cosÃ¬ dette connessioni scorciatoia (shortcut connection).
Per capire la loro utilitÃ  e le esigenze della loro presenza, dobbiamo fare un passo indietro.
Nel nostro post sulle Fully Convolutional Networks abbiamo appreso che nella riduzione delle dimensioni operata dallâ€™encoder la perdita di informazione Ã¨ significativa.
Il decoder fatica quindi a effettuare lâ€™up-sampling con un conseguente risultato mediocre.
Le FCN gestiscono allora la perdita di informazioni ricostruendo lâ€™immagine e recuperando parte delle informazioni dai filtri di pooling prima della sintesi dalla feature map.

Le reti U-Net propongono invece una soluzione differente.
La struttura base rimane invariata, con due percorsi simmetrici che chiamiamo:
- Encoder, o percorso di contrazione (contraction path) che cattura il contesto dellâ€™immagine. Ãˆ costituito da livelli di convoluzione e di max pooling al pari di una Convolutional Neural Network.
- Decoder, o percorso di espansione (expanding path) che localizza con precisione gli elementi dellâ€™immagine attraverso le convoluzioni trasposte (transposed convolutions).
Ogni up-sampling layer del decoder riceve i dati dal corrispondete down sampling layer dellâ€™encoder.

In questo modo siamo in grado di catturare piÃ¹ informazioni contenendo la complessitÃ  computazionale.

Chiaramente i livelli iniziali dellâ€™encoder contengono piÃ¹ informazioni, ergo per cui garantiscono un significativo boost nel processo di up-sampling permettendo il recupero di dettagli e migliorando significativamente il risultato.
Ecco che introduciamo le shortcut connection.
PiÃ¹ nello specifico la prima shortcut connections crea un ponte tra lâ€™encoder prima delâ€™iniziale filtro di pooling e il decoder dopo lâ€™ultima operazione di deconvoluzione.

-----------

Initially proposed as a solution to medical image segmentation problems, but it's now used in many other tasks. 
It's particularly effective in tasks with high resolution inputs and outputs (segmantation, superresolution, diffusion models, denoising).
They are all tasks from image to image.

We generally need a set of inputs-labels to train the U-Net model. The training works as usual, randomizing parameters of the network, checking output compared to grand truth and
finding the parameters of the U-Net that minimize the loss.

ENCODER: extract features from the input image
DECODER: upsampling intermediate features and produce the final output

They are simmetrical and connected => gives the U form.

Basically: I want to find X thing => I figure that this tile of the image probably has it => let me define pixel-wise what is the X thing and what is not 

Connecting parts allow for the encoder features to be concatenated on to the decoder features. They are:
- The bottleneck -> where the encoder switches to decoder => we have the biggest amount of features 

- The connecting paths ("Skip Connections") -> they simply cope the simmetrical feature on the encoder part and concatenate on to the opposing stage on the decoder.
    Decoder features include semantic information (there's a bike around there)
    Encoder features include spatial information (these are pixels of an object)

Key points to compare to a Fully Convolutional Networks:
 - Symmetrical parts -> FCN usually has one or a couple decoder layers, U-Net has as many as the encoder
 - Deconvolution (transposed convolution) is the standard for upsamplign in decoders in U-Nets, while FCN can use interpolation techniques and only sum the pooling layers.

Performance and Data:

+ Low amounts of data is needed => the images are transformed making the model stronger at recognizing objects in different forms/rotations
 U-Nets Require Less Data: It is true that U-Nets can achieve high performance with relatively smaller datasets compared to many other architectures, including Fully Convolutional Networks (FCNs). This capability is largely attributed to their architecture, which includes skip connections that help retain spatial information and enable the model to learn more robust features from fewer examples. Additionally, U-Nets often employ extensive data augmentation techniques, which artificially increase the size of the training dataset by creating variations of the existing images. This allows the model to generalize better without needing a vast number of annotated samples

+ FCNs, while capable of functioning with smaller datasets, may require more data to achieve similar performance levels due to their less structured approach to feature combination.
In terms of speed, both architectures can be efficient, but U-Nets often have an edge in processing speed due to their design tailored for segmentation tasks

The Loss Function is minimized by an Optimizer. One of the most used ones is ADAM.

[Check out 
https://arxiv.org/pdf/1505.04597v1
https://www.andreaprovino.it/u-net
https://stackoverflow.com/questions/50239795/intuition-behind-u-net-vs-fcn-for-semantic-segmentation
]

### 6. N2V

> IDEA: Apply Denoising with no data other than the noisy image we want to denoise.

Here, we introduce
NOISE2VOID (N2V), a training scheme that takes this idea
one step further. It does not require noisy image pairs, nor
clean target images. Consequently, N2V allows us to train
directly on the body of data to be denoised and can therefore
be applied when other methods cannot. Especially interesting is the application to biomedical image data, where
the acquisition of training targets, clean or noisy, is frequently not possible.

N2N training requires the availability of pairs of noisy images, and
the acquisition of such pairs with (quasi) constant s is
only possible for (quasi) static scenes.

While it cannot be expected that our approach outperforms methods that
have additional information available during training, we
observe that the denoising performance of our results only
drops moderately and is still outperforming BM3D.

N2V is a self-supervised training method. In this work
we make two simple statistical assumptions: 
(i) the signal s is not pixel-wise independent, 
(ii) the noise n is conditionally pixel-wise independent given the signal s

In other words, if we were to acquire multiple images with
the same signal, but different realizations of noise and average them, the result would approach the true signal

Another assumption for mathematical proof of the algorithm: E[N] = 0, the expected value of the noise is zero.
This allows to generate clean data from only noisy images.
When E[N]=0 is a Valid Assumption:
Gaussian Noise:
Many denoising frameworks assume Gaussian noise, where noise is drawn from a normal distribution 
ð‘âˆ¼ð‘(0,ðœŽ2)
In this case:
E[N]=0
This is a common model because it works well in many practical settings, such as sensor noise in images.

Random, Uncorrelated Noise:
Even if the noise isnâ€™t strictly Gaussian, it might still be random and centered around zero, especially in systems where errors arise from many independent small effects (per the Central Limit Theorem).

Applications That Justify It:
In microscopy or imaging systems, noise often arises from physical processes (e.g., photon counting noise, electronic sensor noise) and is modeled effectively as zero-mean.

Here, we go a step further. We propose to derive both
parts of our training sample, the input and the target, from
a single noisy training image x
j
. If we were to simply extract a patch as input and use its center pixel as target, our
network would just learn the identity, by directly mapping
the value at the center of the input patch to the output
Figure 2 a).


-----------

The Training:


To understand how training from single noisy images is
possible nonetheless, let us assume that we use a network
architecture with a special receptive field. We assume the
receptive field xËœRF(i) of this network to have a blind-spot
in its center. The CNN prediction sË†i for a pixel is affected
2132
by all input pixels in a square neighborhood except for the
input pixel xi at its very location. We term this type of
network blind-spot network (see Figure 2 b)

The blindspot network has a little bit less information available for
its predictions, and we can expect its accuracy to be slightly
impaired compared to a normal network
The essential advantage of the blind-spot architecture is
its inability to learn the identity. Let us consider why this
is the case. Since we assume the noise to be pixel-wise
independent given the signal (see Eq. 3), the neighboring
pixels carry no information about the value of ni
. It is thus
impossible for the network to produce an estimate that is
better than its a priori expected value (see Eq. 4).
The signal however is assumed to contain statistical dependencies (see Eq. 2). As a result, the network can still
estimate the signal si of a pixel by looking at its surroundings.

We have seen that a blind-spot network can in principle be trained using only individual noisy training images.
However, implementing such a network that can still operate efficiently is not trivial. We propose a masking scheme
to avoid this problem and achieve the same properties with
any standard CNN: We replace the value in the center of
each input patch with a randomly selected value form the
surrounding area (see supplementary material for details).
This effectively erases the pixelâ€™s information and prevents
the network from learning the identity.

If we implement the above training scheme naively, it
is unfortunately still not very efficient: We have to process
an entire patch to calculate the gradients for a single output pixel. To mitigate this issue, we use the following approximation technique: Given a noisy training image xi
, we randomly extract patches of size 64 Ã— 64 pixels, which are
bigger than our networks receptive field (see supplementary
material for details). Within each patch we randomly select
N pixels, using stratified sampling to avoid clustering. We
then mask these pixels and use the original noisy input values as targets at their position (see Figure 3). Further details
on the masking scheme can be found in the supplementary
note. We can now simultaneously calculate the gradients
for all of them, while ignoring the rest of the predicted image. 

-------

Other sources:

Objective
The goal of N2V is to predict a clean signal using a noisy image as both the input and the target, without relying on a paired clean image. To make this work, N2V uses masking and stratified sampling to train efficiently while ensuring the network does not overfit to the noise.

Key Challenges
Inefficiency:
Naively training the network to compute gradients for a single pixel (output) requires processing the entire patch, which is computationally expensive.
Risk of Overfitting to Noise:
Using the same pixel for input and target may allow the network to "memorize" noise instead of denoising.
The Training Scheme
Extracting Random Patches:

From a noisy image xi, random patches of size 
64Ã—64 are sampled during training.
These patches must be larger than the network's receptive field to ensure that the network's predictions for each pixel only depend on information within the patch. This prevents "leakage" of masked information back into the predictions.
Stratified Sampling:

Within each patch, a subset of 
N pixels is chosen randomly for training.
Stratified sampling ensures that the selected pixels are distributed across the patch rather than being clustered together. This improves generalization by training on a diverse set of spatial locations.
Masking Scheme:

The 
N selected pixels are masked during training, meaning their values are replaced with dummy values (often zero or a neighboring pixel's value) in the input patch.
The network's goal is to predict the original (noisy) value of the masked pixels, which acts as the target.
Masking forces the network to infer the missing pixel value based on its neighbors, effectively learning to model the underlying clean signal.
Gradient Calculation:

The network predicts the values for all 
N masked pixels simultaneously, allowing for efficient gradient computation.
The loss is only calculated for the 
N masked pixels, ignoring the rest of the patch.
Batch-Wise Processing:

This approach allows multiple masked pixels in a single patch to be used for training in parallel, significantly improving efficiency compared to processing each pixel individually.
Why Masking and Stratified Sampling?
Masking ensures the network doesn't "see" the original value of a pixel, so it cannot directly memorize noise. Instead, the network learns to use neighboring information to estimate the clean signal.
Stratified sampling avoids redundant information (e.g., clustered pixels) and ensures diverse gradient updates.

Benefits of This Approach
Efficiency:
By masking 
N pixels in one patch, the network processes them simultaneously, making the computation faster.
Robust Learning:
Using stratified sampling and diverse patches ensures the network learns a generalized mapping and does not overfit to specific noise patterns.

You are correct that in the Noise2Void (N2V) framework, the original noisy value of the masked pixel is used as the "ground truth" during training. This raises an important question: if the pixel's value is part of the noise, does the model inadvertently learn to reproduce the noise?
Since the expectation of the noise is zero, over many examples, the network learns to infer the clean signal 
x*, rather than reproducing the noise.

As opposed to supervised approaches, where the loss is computed with respect to a known ground truth, N2V computes the loss based on a noisy signal. At the beginning of the training the network usually learns within a few epochs an approximation of the structures in the image and the loss decreases sharply. Then, it rapidly reaches a plateau and oscillates around a particular loss value.

Because the loss is computed between prediction and noisy signal, its absolute value is not informative for whether the network is properly trained. Likewise, oscillation on the plateau does not indicate that it does not learn anymore.
The best way to assess the quality of the training is to look at the denoised images, potentially at different points during training, using the checkpoints. In practice, we often simply train long enough for the image to look properly denoised!


Why do N2V predictions sometimes look blurry?
The absence of noise can by itself make images look slightly blurry. If some regions of your data look unreasonably blurry, you might be encountering the a case of regression to the mean.

For each noisy image, we often say that there is a whole distribution of possible denoised images. In particular, for strongly degraded images, even different structures can produce the same noisy patch.

Because of this, a network trained with the mean squared error, such as N2V, will tend to predict the average of all possible denoised images. This is why the prediction can often look blurry and washed out. For approaches that predict single instances from the distribution of possible denoised images, check out DivNoising or HDN.

Intuition for N2V Behavior
The masked pixels are used to guide the network to understand the general noise-to-signal mapping.
Over time, as the network learns, its predictions for both masked and unmasked pixels improve.
By the end of training, the network can denoise the entire image effectively, even though the loss was calculated only on a subset of pixels.


[Check out
https://openaccess.thecvf.com/content_CVPR_2019/papers/Krull_Noise2Void_-_Learning_Denoising_From_Single_Noisy_Images_CVPR_2019_paper.pdf
https://careamics.github.io/0.1/algorithms/Noise2Void/
]


## Il Nostro Modello per N2V Denoising

### Configurazione:

{'algorithm_config': {'algorithm': 'n2v',
                      'loss': 'n2v',
                      'lr_scheduler': {'name': 'ReduceLROnPlateau',
                                       'parameters': {}},
                      'model': {'architecture': 'UNet',
                                'conv_dims': 2,
                                'depth': 2,
                                'final_activation': 'None',
                                'in_channels': 4,
                                'independent_channels': True,
                                'n2v2': False,
                                'num_channels_init': 32,
                                'num_classes': 4},
                      'optimizer': {'name': 'Adam',
                                    'parameters': {'lr': 0.0001}}},
 'data_config': {'axes': 'SCYX',
                 'batch_size': 32,
                 'data_type': 'array',
                 'patch_size': [64, 64],
                 'transforms': [{'flip_x': True,
                                 'flip_y': True,
                                 'name': 'XYFlip',
                                 'p': 0.5},
                                {'name': 'XYRandomRotate90', 'p': 0.5},
                                {'masked_pixel_percentage': 0.2,
                                 'name': 'N2VManipulate',
                                 'roi_size': 11,
                                 'strategy': 'uniform',
                                 'struct_mask_axis': 'none',
                                 'struct_mask_span': 5}]},
 'experiment_name': 'n2v_jump_cell_painting_chwise',
 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,
                                             'mode': 'min',
                                             'monitor': 'val_loss',
                                             'save_last': True,
                                             'save_top_k': 3,
                                             'save_weights_only': False,
                                             'verbose': False},
                     'num_epochs': 100},
 'version': '0.1.0'}

ASPETTI DEGNI DI NOTA:

1. Optimizer & LR-Scheduler
2. Model
3. DataConfig
4. TrainingConfig

#### 1. Adam Optimizer con ReduceLROnPlateau

Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.

Tutti gli Ottimizzatori si basano sull'Algoritmo di Discesa del Gradiente. Data una funzione (nel nostro caso la Loss Function, ad esempio una MSE) avente un certo numero di incognite (nel nostro caso le incognite sono i parametri del modello), vogliamo trovare i valori per quelle incognite che minimizzano la funzione.
Intuitivamente il minimo richiede il calcolo della derivata prima, dunque l'algoritmo procede cosÃ¬:
- Inizia da un punto a caso x0.
- Calcola il gradiente nel punto x0, ovvero calcola la derivata prima della funzione in x0. 
    Se gradiente > 0, la funzione sta salendo in quel punto: vogliamo ridurre x0, spostandoci verso sinistra.
    Se gradiente < 0, la funzione sta scendendo in quel punto: vogliamo aumentare x0, spostandoci verso destra.
- x1 Ã¨ calcolata come x1 = x0 - mu * gradient(x0), dove mu Ã¨ il Learning Rate.
    Un Learning Rate alto significa che la nostra discesa del gradiente fa grandi salti ad ogni iterazione, convergendo piÃ¹ velocemente ma potenzialmente mancando il punto di minimo.
    Un Learning Rate basso significa che la nostra discesa del gradiente fa piccoli salti ad ogni iterazione, convergendo piÃ¹ lentamente ma garantendo, nel tempo, un'approssimazione migliore del minimo.

L'Algoritmo puÃ² terminare in due modi:
- La differenza tra x_i+1 ed x_i Ã¨ molto bassa. Si sospetta in questo caso che si sia giunti al minimo.
- L'Algoritmo termina il numero di iterazioni a sua disposizione.

Nel caso di funzioni a piÃ¹ parametri, si utilizzano Derivate Parziali.

Gli Ottimizzatori si distinguono per il modo in cui implementano l'Algoritmo di Gradient Descent. 
Una misura che li differnzia Ã¨ quando questo algoritmo viene eseguito, durante il training:
- Se viene eseguito una sola volta, quando tutti gli input sono stati passati nella rete, si tratta di una Discesa del Gradiente "tradizionale" o "Batch Gradient Descent" (BGD).
 Il suo problema Ã¨, a livello computazionale, una alta richiesta di memoria, mentre in generale rappresenta una rete chiaramente poco dinamica (cambia solo alla fine i suoi parametri)
- Se viene eseguito ad ogni input, si tratta di una Discesa del Gradiente Stocastica (SGD). Dobbiamo in questo caso adattare un po' l'algoritmo, poichÃ© se esso converge al minimo ad ogni esecuzione
 finiamo per trovare i parametri che minimizzano solamente l'ultimo input. Per farlo si utilizzano:
    - parametri aggiuntivi, che osservano come il gradiente varia (intutivamente, possiamo immaginare derivazioni
        di secondo grado)
    - learning rate variabile. Intuitivamente, un LR piÃ¹ piccolo su un numero limitato di iterazioni consente, in una esecuzione dell'algoritmo,
        solo una certa variazione contenuta al valore dei parametri. Man mano che andiamo avanti, riduciamo il Learning Rate consentendo alla rete di stabilizzarsi.
- Se viene eseguito ogni volta che un certo numero di input viene passato nella rete, si tratta di una "Mini-Batch Gradient Descent".
 Sono necessarie anche qui tutte le tecniche descritte in precedenza. La dimensione del "mini_batch" Ã¨ specificata tipicamente come batch_size nell'Algoritmo.
 La scelta del Batch Size influenza le prestazioni dell'Algoritmo e deve avvenire coerentemente con la quantitÃ  di memoria RAM disponibile nel sistema.

 .... Ci sono tutta una serie di Ottimizzatori che introducono parametri aggiuntivi e fanno cose complicate ....

L'Ottimizzatore piÃ¹ utilizzato, riconosciuto come migliore, Ã¨ l'Adam Optimizer. 
ADAM = Adaptive Momentum Estimation
In molta sintesi, poichÃ© i dettagli non c'Ã¨ tempo di presentarli, combina questi comportamenti:
- Mini-Batch, dunque viene eseguito ogni batch_size.
- "Momentum", It accelerates the convergence towards the relevant direction and reduces the fluctuation to the irrelevant direction.
    Utilizzando un parametro gamma di "Momentum", che agisce cosÃ¬ nella formula di aggiornamento:
     x_i+1 = gamma*x_i + lr \* gradient(x_i)
    In pratica gamma = 0.9 circa e dunque x_i ha una importanza leggermente ridotta, dando piÃ¹ importanza all'ultima rilevazione.
- Learning Rate adattivo, per i motivi descritti sopra. In particolare ogni parametro ha un proprio Learning Rate, rendendo la cosa ancora piÃ¹ accurata.

Nel concreto, ADAM si porta in giro due Medie Mobili (Moving Averages), degli ultimi gradienti e degli ultimi gradienti^2. 
Tramite queste due Moving Averages applica il Momentum e stabilisce come variare il Learning Rate per ogni parametro. 

++ Efficient and Scalable:

Works well for large datasets and high-dimensional parameter spaces.
Performs well out-of-the-box with minimal hyperparameter tuning.

LRPLATEAU:
Nella nostra configurazione, l'LR_Scheduler si occupa di stabilire come deve variare il Learning Rate. Intuitivamente quando un certo parametro raggiunge un "plateau", l'LR Ã¨ ridotto.

Monitor Progress: The scheduler tracks a specific metric (e.g., validation loss).
Detect Plateaus: If the metric doesnâ€™t improve for a specified number of epochs (the "patience"), the scheduler reduces the learning rate by a factor.
Improve Training Stability: A lower learning rate allows for more fine-tuned adjustments to parameters, especially helpful during the later stages of training when the model is close to convergence.

Example Scenario
Letâ€™s say youâ€™re training a neural network:

Initial learning rate: 0.01
Patience: 3 epochs
Factor: 0.1
If the validation loss hasnâ€™t improved after 3 consecutive epochs, the learning rate will be reduced to 0.01Ã—0.1=0.001.


[Check out 
https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6 
https://arxiv.org/pdf/1412.6980
]

#### 2. Model

'architecture': 'UNet' -> Model is a U-Net, for the reasons we know of.

'conv_dims': 2 -> The Model performs 2D Convolutions, which is expected when working with 2D input signals like images.

'depth': 2 -> The Model performs 2 levels of down-sampling in the encoder and 2 corresponding up-sampling levels in the decoder. This is standard for the U-Nets and makes
    for a compact and efficient network. More layers are only needed if there's need to capture more complex featuresm which is not our case.

'final_activation': 'None' -> At the end of the processing, the Model doesn't apply any Activation Function. This is tipical for tasks like denoising, as the result is an image and not a prediction.
    In neural networks that need to give a probabilistic prediction as output (between 0 and 1), a Final Activation function like Sigmoid or Softmax is usually applied at the end.

'in_channels': 4 -> The 4 channels that describe our dataset images (RGB + A)
'num_classes': 4 -> Matches the 4 input channels, meaning that denoising is applied to all 4 channels and the result is also 4 channels, denoised
'independent_channels': True -> It means that the channels are processed independently, that is using separate feature maps. Looking at the diversity of our channels, it makes sense to work separately.

'n2v2': False -> The Model applies N2V and not its modified version N2V2 (more on that later?)

'num_channels_init': 32 -> This number corresponds to the feature map size. Tipically, every level the number doubles so we shuold expect

In a U-Net with depth = 2 and num_channels_init = 32, the down-sampling process involves reducing the spatial dimensions of the input while increasing the number of feature channels. Here's how it works step-by-step:

Down-sampling in U-Net (Depth = 2)
Initial Input:

Assume the input is a 2D image with dimensions (H, W) and 4 channels (as in_channels = 4).
Example: Input shape is (H, W, 4).
Initial Convolution:

The input is passed through the first convolutional layer, which applies 32 filters (num_channels_init = 32).

The output feature map has dimensions:

Spatial size: Still (H, W) (if padding is applied to preserve dimensions).
Channels: 32.
After this layer:

Shape: (H, W, 32).
Level 1 (First Down-sampling Step):
Down-sampling:

Down-sampling is typically performed using max-pooling (e.g., with a 2Ã—2 kernel and stride 2).
This reduces the spatial dimensions by half:
Spatial size: From (H, W) â†’ (H/2, W/2).
The number of channels remains the same: 32.
Convolution:

Another convolutional layer is applied after down-sampling.
The number of channels increases (doubles) as per U-Net design:
From 32 â†’ 64.
Output of Level 1:

Shape: (H/2, W/2, 64).
Level 2 (Second Down-sampling Step):
Down-sampling:

Max-pooling is applied again, halving the spatial dimensions:
From (H/2, W/2) â†’ (H/4, W/4).
The number of channels remains 64.
Convolution:

Another convolutional layer is applied, doubling the channels again:
From 64 â†’ 128.
Output of Level 2:

Shape: (H/4, W/4, 128).

Summary of Down-sampling Stages
Level	    Spatial Dimensions	     Channels
Input	    (H, W)	                 4
Level 0	    (H, W)	                 32
Level 1	    (H/2, W/2)	             64
Level 2	    (H/4, W/4)	             128

#### 3. DataConfig

'axes': 'SCYX' -> This is the shape of our training data:
    S = Sample Axis, aka the number of imags
    C = Channel Axis, aka the number of channels
    Y = Spatial Height of the image
    X = Spatial Width of the image
Our Training Data has a shape of: (413, 4, 540, 540)
Our Validation Data has a shape of: (104, 4, 540, 540)

'batch_size': 32 -> This is the number of samples per batches, so the number of images that are being processed in parallel during a single pass, before we update the model params (see optmizer)
    It's one of the numbers we manually chose when running the training for the model. 32 is a balanced, common choice when memory is somewhat limited.

'data_type': 'array' -> This is simply the format of the data, in our case numpy arrays.

'patch_size' : [64, 64] -> This defines the size of the patches in which the input images are divided during processing. It's an usual practice for training as it reduces memory requirements and
    allows for a more local processing. The choice of 64x64 is in line with the patch size specified for masking in the official N2V Algorithm document

TRANSFORMATIONS: U-Nets apply data augmentation techniques to train the model's robustness and generalization. In our configs, transforms describe the operations the input images are subject to before the processing.
    In our case:
        1. As standard for U-Nets, we flip on both axis. This increases the variety of input images without needing more data.
        2. As standard for U-Nets, we rotate the image by 0, 90, 180 or 270 degrees. Rotations introduce orientation invariance, helping the model generalize better.
        3. N2VManipulate is the specific masking transformation described in the N2V Algoritm. Details are below

            N2VManipulate: This is a key transformation specific to the Noise2Void (N2V) approach.
            masked_pixel_percentage: 0.2
            During training, 20% of the pixels in the patch are masked (replaced) for the self-supervised learning task.
            roi_size: 11
            Refers to the size of the Region of Interest (ROI) around the masked pixel that is used as context for predicting the missing value.
            strategy: 'uniform'
            Specifies how pixels are selected for masking:
            Uniform means pixels are chosen randomly from the patch.
            struct_mask_axis: 'none'
            Indicates that no specific axis is structured for masking (i.e., masking is isotropic/random).
            struct_mask_span: 5
            Defines the span of structured masking if struct_mask_axis were set (unused here due to 'none').


#### 4. TrainingConfig

This is the least interesting part of the configuration. It explains the form of the output of the training, in particular its checkpoints.
Basically, a certain metric (Val_loss, calculated on the Validation Dataset) is monitored and when it reaches a local minimum the state of the model (that is, the value of its params)
is saved in a "checkpoint" file. 
From the training are saved the best 3 checkpoints + the final model

'num_epochs': 100 -> This is the number of complete passes, that is the number of times the entire dataset is passed through the network for training.
    This is the second parameter we manually specify, as it depends on how much time you are willing to wait for a result and how performant you want your model to be



## Run our Model on BSD68 to see what happens

Reference: https://careamics.github.io/0.1/applications/Noise2Void/BSD68/
https://github.com/bnsreenu/python_for_microscopists/blob/master/293_denoising_RGB_images_using_deep%20learning.ipynb

I want to:

- Create a library folder with ALL and ONLY the functions I need for the Notebooks
- Create a Notebook for the training and prediction of the JUMP Cell Database
- Create a Notebook for the training and prediction of the BSD68 Training
- Create a Notebook for the training and prediction of 1 single BSD68 image, for presentation


## Create a Small DEMO for a single image and a model trained on it on a Jupiter Notebook

